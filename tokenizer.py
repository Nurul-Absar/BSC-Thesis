# Define the custom tokenizer function
def custom_tokenizer(text):
    return text.split()